{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7616289f-a1f0-443c-a72e-4dfe39572560",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING SPLIT IMAGES\n",
      "\n",
      "Train: Datset is not Matching\n",
      "Valid: Datset is not Matching\n",
      "Test: Datset is not Matching\n",
      "\n",
      "AUGMENTING TRAINING IMAGES\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fe6e8711c24b31a58a2680f6dde2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/561 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/albumentations/augmentations/functional.py:485: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n",
      "  warn(\n",
      "/usr/lib/python3/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUGMENTING VALIDATION IMAGES\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574bd2b916e04c5ba7cf665c4e03dba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUGMENTING TEST IMAGES\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bc45d706f845eda48520a6214ae806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## IMPORT ALL LIBRAIES...\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader \n",
    "import torchvision.models as models\n",
    "from ImageProcess import SplitImages, checkingsplitsame\n",
    "from ImageClfModeling import set_parameter_requires_grad, train_model\n",
    "\n",
    "\n",
    "#--------------------------------------------IMAGE PATH---------------------------------------------\n",
    "\n",
    "# image path\n",
    "imagepath = \"Image/Images/\"\n",
    "\n",
    "\n",
    "#--------------------------------------------INITIAL GLOBAL TRANSFORMATION---------------------------------------------\n",
    "# global transformation into tensor form \n",
    "global_transform = Compose([transforms.ToTensor()])\n",
    "database = ImageFolder(root=imagepath, transform=global_transform)\n",
    "\n",
    "\n",
    "#--------------------------------------------CLASS LABELS TO INDEX---------------------------------------------\n",
    "\n",
    "# class names from index to class\n",
    "class_names = [x for x in database.classes]\n",
    "class_name_dict = {}\n",
    "for ix, val in enumerate(class_names):\n",
    "    class_name_dict[ix] = val\n",
    "    \n",
    "    \n",
    "#--------------------------------------------RANDOMIZED IMAGE SPLIT---------------------------------------------\n",
    "\n",
    "# random images split \n",
    "valid_div, test_div, randomseed = 20, 10, 10\n",
    "valid_sz = len(database)//valid_div\n",
    "test_sz = len(database)//test_div\n",
    "train_ds, valid_ds, test_ds = SplitImages(database, valid_sz, test_sz, randomseed)\n",
    "\n",
    "\n",
    "#--------------------------------------------SPLIT IMAGES TEST---------------------------------------------\n",
    "print(\"TESTING SPLIT IMAGES\")\n",
    "print()\n",
    "# testing indices for test data with train data \n",
    "checkingsplitsame(test_ds, train_ds, \"Train\")\n",
    "\n",
    "# testing indices for test data with validation data \n",
    "checkingsplitsame(valid_ds, test_ds, \"Valid\")\n",
    "\n",
    "# testing indices for validation data with train data \n",
    "checkingsplitsame(valid_ds, train_ds, \"Test\")\n",
    "\n",
    "\n",
    "#--------------------------------------------DATA AUGMENTATION---------------------------------------------\n",
    "\n",
    "# train transformation \n",
    "print()\n",
    "print(\"AUGMENTING TRAINING IMAGES\")\n",
    "print()\n",
    "train_transform = A.Compose([A.HorizontalFlip(p=0.5), \n",
    "                            A.ImageCompression(quality_lower=99, quality_upper=100),\n",
    "                            A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=10,\n",
    "                                              border_mode=0, p=0.7),\n",
    "                            A.Resize(224, 224),\n",
    "                            A.Cutout(max_h_size=int(224*0.4), max_w_size=int(224*0.4), \n",
    "                                    num_holes=1, p=0.65),\n",
    "                            ToTensorV2()])\n",
    "\n",
    "traindata_trans = []\n",
    "\n",
    "for img, lb in tqdm(train_ds):\n",
    "    im = train_transform(image=img.permute(1,2,0).numpy())\n",
    "    traindata_trans.append((im[\"image\"], lb))\n",
    "    \n",
    "    \n",
    "# validation transformation \n",
    "print()\n",
    "print(\"AUGMENTING VALIDATION IMAGES\")\n",
    "print()\n",
    "valid_transform = A.Compose([A.Resize(224, 244),\n",
    "                            ToTensorV2()])\n",
    "\n",
    "validdata_trans = []\n",
    "\n",
    "for img, lb in tqdm(valid_ds):\n",
    "    im = valid_transform(image = img.permute(1,2,0).numpy())\n",
    "    validdata_trans.append((im[\"image\"], lb))\n",
    "\n",
    "\n",
    "# test data transformation \n",
    "print()\n",
    "print(\"AUGMENTING TEST IMAGES\")\n",
    "print()\n",
    "test_transform = A.Compose([A.Resize(224, 244),\n",
    "                            ToTensorV2()])\n",
    "\n",
    "testdata_trans = []\n",
    "\n",
    "for img, lb in tqdm(test_ds):\n",
    "    im = test_transform(image = img.permute(1,2,0).numpy())\n",
    "    testdata_trans.append((im[\"image\"], lb))\n",
    "    \n",
    "    \n",
    "#--------------------------------------------DATA LOADERS---------------------------------------------  \n",
    "\n",
    "\n",
    "# batch size for train and validation data \n",
    "batch_size_ = 64\n",
    "\n",
    "# batch size for test data (we want to test all the data at one go)\n",
    "batch_test = len(test_ds)\n",
    "\n",
    "# Iterative Data loader\n",
    "train_dl = DataLoader(traindata_trans, batch_size=batch_size_, shuffle=True, num_workers=2)\n",
    "valid_dl = DataLoader(validdata_trans, batch_size=batch_size_, shuffle=True, num_workers=2)\n",
    "test_dl = DataLoader(testdata_trans, batch_size=batch_test, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af387125-569b-4b96-ae43-378da9167325",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------MODEL---------------------------------------\n",
    "\n",
    "\n",
    "# Initialising the pre trained model\n",
    "'''def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc =  nn.Sequential(nn.Dropout(0.1),nn.Linear(num_ftrs,num_classes))\n",
    "    return model_ft'''\n",
    "\n",
    "\n",
    "def initialize_model(num_classes, feature_extract, use_pretrained=True):\n",
    "    model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "    set_parameter_requires_grad(model_ft, feature_extract)\n",
    "    model_last_inp = model_ft.classifier[6].in_features\n",
    "    model_ft.classifier[6] = nn.Linear(model_last_inp, num_classes)\n",
    "    return model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7031ff-6529-4587-8fb5-4c3a5c1306d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=11, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------INITIALIZING PRE TRAINED MODEL-----------------------------------\n",
    "\n",
    "\n",
    "num_classes = len(class_name_dict)\n",
    "feature_extract = False\n",
    "\n",
    "# pre trained model\n",
    "model_ft = initialize_model(num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d59aa1-edda-428b-a31f-d6ec81130975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda device\n",
      "Params to learn:\n",
      "\t features.0.weight\n",
      "\t features.0.bias\n",
      "\t features.3.weight\n",
      "\t features.3.bias\n",
      "\t features.6.weight\n",
      "\t features.6.bias\n",
      "\t features.8.weight\n",
      "\t features.8.bias\n",
      "\t features.10.weight\n",
      "\t features.10.bias\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n",
      "\t classifier.4.weight\n",
      "\t classifier.4.bias\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------SENDING MODEL TO GPU-------------------------------------\n",
    "\n",
    "\n",
    "# Send the model to GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "print(f'using {device} device')\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "\n",
    "for name,param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164da3ea-9165-4579-bb1a-16ced8272c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------MODEL PARAMETERS---------------------------------------------\n",
    "\n",
    "\n",
    "# Epochs\n",
    "epochs_ = 50\n",
    "\n",
    "# learning rate\n",
    "lr= 1e-3\n",
    "\n",
    "# momentum\n",
    "moment = 0.9\n",
    "\n",
    "# Optimizer\n",
    "optimizer_ft = torch.optim.SGD(params_to_update, lr=lr, momentum=moment)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00079d3f-a43e-4921-ac74-d9af04578168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0| train loss: 136.30084228515625| valid loss: 50.32771301269531| train acc: 0.504960298538208| valid acc: 0.9090909361839294\n",
      "Epoch: 1| train loss: 77.51609802246094| valid loss: 30.032184600830078| train acc: 0.9017148613929749| valid acc: 0.8787878751754761\n",
      "Epoch: 2| train loss: 54.11399841308594| valid loss: 28.930511474609375| train acc: 0.9284297227859497| valid acc: 0.9393939971923828\n",
      "Epoch: 3| train loss: 40.85371780395508| valid loss: 25.552658081054688| train acc: 0.959537923336029| valid acc: 0.9393939971923828\n",
      "Epoch: 4| train loss: 31.029998779296875| valid loss: 31.859148025512695| train acc: 0.975162923336029| valid acc: 0.9393939971923828\n",
      "Epoch: 5| train loss: 22.94483184814453| valid loss: 24.445480346679688| train acc: 0.9965277910232544| valid acc: 0.9393939971923828\n",
      "Epoch: 6| train loss: 15.99258804321289| valid loss: 20.68382453918457| train acc: 0.9982638955116272| valid acc: 0.9393939971923828\n",
      "Epoch: 7| train loss: 13.619832038879395| valid loss: 33.42455291748047| train acc: 0.9982638955116272| valid acc: 0.9393939971923828\n",
      "Epoch: 8| train loss: 12.653884887695312| valid loss: 21.978782653808594| train acc: 0.9965277910232544| valid acc: 1.0\n",
      "Epoch: 9| train loss: 9.622933387756348| valid loss: 26.66250228881836| train acc: 0.9982638955116272| valid acc: 0.9393939971923828\n",
      "Epoch: 10| train loss: 7.1080474853515625| valid loss: 25.810325622558594| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 11| train loss: 6.1356306076049805| valid loss: 26.41884422302246| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 12| train loss: 5.968127250671387| valid loss: 21.998868942260742| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 13| train loss: 4.075625419616699| valid loss: 29.89789581298828| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 14| train loss: 3.69128155708313| valid loss: 31.218231201171875| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 15| train loss: 3.8369321823120117| valid loss: 30.453777313232422| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 16| train loss: 3.4060440063476562| valid loss: 28.546085357666016| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 17| train loss: 3.757587432861328| valid loss: 24.40746307373047| train acc: 1.0| valid acc: 1.0\n",
      "Epoch: 18| train loss: 2.887974262237549| valid loss: 31.80380630493164| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 19| train loss: 2.6353769302368164| valid loss: 24.419967651367188| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 20| train loss: 2.1531600952148438| valid loss: 20.330148696899414| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 21| train loss: 1.6036112308502197| valid loss: 26.913497924804688| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 22| train loss: 1.681014060974121| valid loss: 31.04581069946289| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 23| train loss: 1.6374571323394775| valid loss: 40.898765563964844| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 24| train loss: 2.0200140476226807| valid loss: 18.658641815185547| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 25| train loss: 2.1737921237945557| valid loss: 31.99753189086914| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 26| train loss: 1.1597561836242676| valid loss: 28.227331161499023| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 27| train loss: 1.2508320808410645| valid loss: 33.19512176513672| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 28| train loss: 1.1845755577087402| valid loss: 23.548885345458984| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 29| train loss: 1.124855399131775| valid loss: 32.65275573730469| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 30| train loss: 0.8250057697296143| valid loss: 30.586673736572266| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 31| train loss: 1.1000205278396606| valid loss: 32.74001693725586| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 32| train loss: 1.4230490922927856| valid loss: 26.460039138793945| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 33| train loss: 0.8535674214363098| valid loss: 37.27070236206055| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 34| train loss: 1.2630540132522583| valid loss: 28.088882446289062| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 35| train loss: 2.005826234817505| valid loss: 29.2655086517334| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 36| train loss: 1.3031073808670044| valid loss: 32.1785774230957| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 37| train loss: 0.9485706686973572| valid loss: 31.125734329223633| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 38| train loss: 0.7103777527809143| valid loss: 28.281801223754883| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 39| train loss: 0.6146318912506104| valid loss: 28.626394271850586| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 40| train loss: 0.39405059814453125| valid loss: 29.728004455566406| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 41| train loss: 0.4073289930820465| valid loss: 32.293033599853516| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 42| train loss: 0.3389698565006256| valid loss: 31.620073318481445| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 43| train loss: 0.44776731729507446| valid loss: 33.5323371887207| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 44| train loss: 0.5540790557861328| valid loss: 39.40596008300781| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 45| train loss: 0.528069257736206| valid loss: 41.67654800415039| train acc: 1.0| valid acc: 0.9393939971923828\n",
      "Epoch: 46| train loss: 0.44369474053382874| valid loss: 29.250972747802734| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 47| train loss: 0.7506785988807678| valid loss: 32.43508529663086| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 48| train loss: 0.9502679109573364| valid loss: 29.672746658325195| train acc: 1.0| valid acc: 0.9696969985961914\n",
      "Epoch: 49| train loss: 0.5846465826034546| valid loss: 24.45140838623047| train acc: 1.0| valid acc: 0.9696969985961914\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------- TRAIN & VALIDATE--------------------------------------\n",
    "\n",
    "#dataloaders\n",
    "dataloaders_dict ={}\n",
    "dataloaders_dict['train']= train_dl\n",
    "dataloaders_dict['valid'] = valid_dl\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, epoch_loss_train, epoch_acc_train, epoch_loss_valid, epoch_acc_valid  = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, device, num_epochs=epochs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1be975-b7be-4525-8cf7-7bee41f370f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------TEST IMAGES-----------------------\n",
    "\n",
    "# input data \n",
    "for inputs, lbs in test_dl:\n",
    "    if len(inputs)==batch_test:\n",
    "        inputs = inputs.to(device) \n",
    "        model_ft.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs_ = model_ft(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac6c3b7-3140-424d-84f1-fe2ccfc6e5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9848], device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ImageClfModeling import topk_accuracy\n",
    "\n",
    "topk_accuracy(outputs_, lbs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f088011-f72e-402e-8103-bdb761038617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------SAVE MODEL---------------------------\n",
    "\n",
    "# save the labels\n",
    "\n",
    "df_label = pd.DataFrame()\n",
    "df_label[\"Labels\"] = list(class_name_dict.keys())\n",
    "df_label[\"Label_name\"] = list(class_name_dict.values())\n",
    "\n",
    "df_label.to_csv(\"scence_name.csv\", index=False)\n",
    "\n",
    "torch.save(model_ft.state_dict(), \"AlexNet_SceneImageClassificationWeight.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
